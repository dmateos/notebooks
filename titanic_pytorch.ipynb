{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle/input/.DS_Store\n",
      "kaggle/input/titanic/test.csv\n",
      "kaggle/input/titanic/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"kaggle/input/titanic/train.csv\")\n",
    "train_data.head()\n",
    "\n",
    "test_data = pd.read_csv(\"kaggle/input/titanic/test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "model = Model(5, 1024)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  SibSp  Parch\n",
       "0       3    1  22.0      1      0\n",
       "1       1    0  38.0      1      0\n",
       "2       3    0  26.0      0      0\n",
       "3       1    0  35.0      1      0\n",
       "4       3    1  35.0      0      0"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prep_data(data):\n",
    "    X_Train = data.drop([\"Name\"], axis=1)\n",
    "    X_Train = X_Train.drop([\"Ticket\"], axis=1)\n",
    "    X_Train = X_Train.drop([\"Cabin\"], axis=1)\n",
    "    X_Train = X_Train.drop([\"Embarked\"], axis=1)\n",
    "    X_Train = X_Train.drop([\"PassengerId\"], axis=1)\n",
    "    X_Train = X_Train.drop([\"Fare\"], axis=1)\n",
    "    X_Train[\"Age\"] = X_Train[\"Age\"].fillna(X_Train[\"Age\"].mean())\n",
    "    try:\n",
    "        X_Train = X_Train.drop([\"Survived\"], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    X_Train[\"Sex\"] = le.fit_transform(X_Train[\"Sex\"])\n",
    "    #X_Train[\"Age\"] = le.fit_transform(X_Train[\"Age\"])\n",
    "    return X_Train\n",
    "\n",
    "X_Train = prep_data(train_data)\n",
    "X_Test = prep_data(test_data)\n",
    "y = train_data[\"Survived\"]\n",
    "\n",
    "X_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0000,  1.0000, 22.0000,  1.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000, 38.0000,  1.0000,  0.0000],\n",
      "        [ 3.0000,  0.0000, 26.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 3.0000,  0.0000, 29.6991,  1.0000,  2.0000],\n",
      "        [ 1.0000,  1.0000, 26.0000,  0.0000,  0.0000],\n",
      "        [ 3.0000,  1.0000, 32.0000,  0.0000,  0.0000]])\n",
      "Epoch:  0 Loss:  0.2670636475086212\n",
      "Epoch:  1 Loss:  0.14677417278289795\n",
      "Epoch:  2 Loss:  0.27308329939842224\n",
      "Epoch:  3 Loss:  0.22301054000854492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/xcg0d2_s6tg_2v3r53qlxyl40000gn/T/ipykernel_1681/2265104275.py:15: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = nn.functional.mse_loss(y_pred, y_test)\n",
      "/var/folders/s0/xcg0d2_s6tg_2v3r53qlxyl40000gn/T/ipykernel_1681/2265104275.py:15: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = nn.functional.mse_loss(y_pred, y_test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 Loss:  0.2740204632282257\n",
      "Epoch:  5 Loss:  0.23996655642986298\n",
      "Epoch:  6 Loss:  0.2247389405965805\n",
      "Epoch:  7 Loss:  0.2236030399799347\n",
      "Epoch:  8 Loss:  0.2308950424194336\n",
      "Epoch:  9 Loss:  0.2238209992647171\n",
      "Epoch:  10 Loss:  0.22854161262512207\n",
      "Epoch:  11 Loss:  0.22547949850559235\n",
      "Epoch:  12 Loss:  0.0845305547118187\n",
      "Epoch:  13 Loss:  0.23388749361038208\n",
      "Epoch:  14 Loss:  0.12703879177570343\n",
      "Epoch:  15 Loss:  0.2260761857032776\n",
      "Epoch:  16 Loss:  0.07092224061489105\n",
      "Epoch:  17 Loss:  0.2885846197605133\n",
      "Epoch:  18 Loss:  0.23259319365024567\n",
      "Epoch:  19 Loss:  0.2519058287143707\n",
      "Epoch:  20 Loss:  0.23121346533298492\n",
      "Epoch:  21 Loss:  0.22519728541374207\n",
      "Epoch:  22 Loss:  0.22259241342544556\n",
      "Epoch:  23 Loss:  0.2862231731414795\n",
      "Epoch:  24 Loss:  0.26983800530433655\n",
      "Epoch:  25 Loss:  0.15925420820713043\n",
      "Epoch:  26 Loss:  0.06564989686012268\n",
      "Epoch:  27 Loss:  0.2280551791191101\n",
      "Epoch:  28 Loss:  0.2508012354373932\n",
      "Epoch:  29 Loss:  0.23217371106147766\n",
      "Epoch:  30 Loss:  0.4007066488265991\n",
      "Epoch:  31 Loss:  0.2246631234884262\n",
      "Epoch:  32 Loss:  0.23692557215690613\n",
      "Epoch:  33 Loss:  0.22517424821853638\n",
      "Epoch:  34 Loss:  0.31909075379371643\n",
      "Epoch:  35 Loss:  0.33442121744155884\n",
      "Epoch:  36 Loss:  0.09939754009246826\n",
      "Epoch:  37 Loss:  0.22889648377895355\n",
      "Epoch:  38 Loss:  0.2228110134601593\n",
      "Epoch:  39 Loss:  0.20252704620361328\n",
      "Epoch:  40 Loss:  0.22452057898044586\n",
      "Epoch:  41 Loss:  0.15887895226478577\n",
      "Epoch:  42 Loss:  0.24348381161689758\n",
      "Epoch:  43 Loss:  0.3083634078502655\n",
      "Epoch:  44 Loss:  0.22718212008476257\n",
      "Epoch:  45 Loss:  0.2702482342720032\n",
      "Epoch:  46 Loss:  0.2234674096107483\n",
      "Epoch:  47 Loss:  0.22317279875278473\n",
      "Epoch:  48 Loss:  0.07594066113233566\n",
      "Epoch:  49 Loss:  0.06037600710988045\n",
      "Epoch:  50 Loss:  0.2232566922903061\n",
      "Epoch:  51 Loss:  0.23414097726345062\n",
      "Epoch:  52 Loss:  0.05475659668445587\n",
      "Epoch:  53 Loss:  0.1162199154496193\n",
      "Epoch:  54 Loss:  0.2514137625694275\n",
      "Epoch:  55 Loss:  0.2804058790206909\n",
      "Epoch:  56 Loss:  0.251248300075531\n",
      "Epoch:  57 Loss:  0.26197871565818787\n",
      "Epoch:  58 Loss:  0.24596288800239563\n",
      "Epoch:  59 Loss:  0.05273048207163811\n",
      "Epoch:  60 Loss:  0.31050390005111694\n",
      "Epoch:  61 Loss:  0.3218313455581665\n",
      "Epoch:  62 Loss:  0.3550489544868469\n",
      "Epoch:  63 Loss:  0.09557173401117325\n",
      "Epoch:  64 Loss:  0.2244071364402771\n",
      "Epoch:  65 Loss:  0.23736068606376648\n",
      "Epoch:  66 Loss:  0.22454135119915009\n",
      "Epoch:  67 Loss:  0.2728321850299835\n",
      "Epoch:  68 Loss:  0.26878514885902405\n",
      "Epoch:  69 Loss:  0.22805404663085938\n",
      "Epoch:  70 Loss:  0.31052398681640625\n",
      "Epoch:  71 Loss:  0.22701117396354675\n",
      "Epoch:  72 Loss:  0.3918479084968567\n",
      "Epoch:  73 Loss:  0.26194480061531067\n",
      "Epoch:  74 Loss:  0.272678017616272\n",
      "Epoch:  75 Loss:  0.045802343636751175\n",
      "Epoch:  76 Loss:  0.27224045991897583\n",
      "Epoch:  77 Loss:  0.6008918285369873\n",
      "Epoch:  78 Loss:  0.23753687739372253\n",
      "Epoch:  79 Loss:  0.23204056918621063\n",
      "Epoch:  80 Loss:  0.26619836688041687\n",
      "Epoch:  81 Loss:  0.0967053696513176\n",
      "Epoch:  82 Loss:  0.29492416977882385\n",
      "Epoch:  83 Loss:  0.3086646795272827\n",
      "Epoch:  84 Loss:  0.37314414978027344\n",
      "Epoch:  85 Loss:  0.2703704833984375\n",
      "Epoch:  86 Loss:  0.1059623584151268\n",
      "Epoch:  87 Loss:  0.2041274905204773\n",
      "Epoch:  88 Loss:  0.23073990643024445\n",
      "Epoch:  89 Loss:  0.25352105498313904\n",
      "Epoch:  90 Loss:  0.22341425716876984\n",
      "Epoch:  91 Loss:  0.22378955781459808\n",
      "Epoch:  92 Loss:  0.23075610399246216\n",
      "Epoch:  93 Loss:  0.2262670248746872\n",
      "Epoch:  94 Loss:  0.1149313747882843\n",
      "Epoch:  95 Loss:  0.2245054990053177\n",
      "Epoch:  96 Loss:  0.24468088150024414\n",
      "Epoch:  97 Loss:  0.10743550211191177\n",
      "Epoch:  98 Loss:  0.22435371577739716\n",
      "Epoch:  99 Loss:  0.07088213413953781\n",
      "Epoch:  100 Loss:  0.12182027846574783\n",
      "Epoch:  101 Loss:  0.2271614670753479\n",
      "Epoch:  102 Loss:  0.04680989682674408\n",
      "Epoch:  103 Loss:  0.25423112511634827\n",
      "Epoch:  104 Loss:  0.2477348893880844\n",
      "Epoch:  105 Loss:  0.23024553060531616\n",
      "Epoch:  106 Loss:  0.3246970772743225\n",
      "Epoch:  107 Loss:  0.07840075343847275\n",
      "Epoch:  108 Loss:  0.23851129412651062\n",
      "Epoch:  109 Loss:  0.05496273934841156\n",
      "Epoch:  110 Loss:  0.22999471426010132\n",
      "Epoch:  111 Loss:  0.07425158470869064\n",
      "Epoch:  112 Loss:  0.23623806238174438\n",
      "Epoch:  113 Loss:  0.0712391659617424\n",
      "Epoch:  114 Loss:  0.22317883372306824\n",
      "Epoch:  115 Loss:  0.2815982401371002\n",
      "Epoch:  116 Loss:  0.29021814465522766\n",
      "Epoch:  117 Loss:  0.24947340786457062\n",
      "Epoch:  118 Loss:  0.33506619930267334\n",
      "Epoch:  119 Loss:  0.1606404334306717\n",
      "Epoch:  120 Loss:  0.3313376009464264\n",
      "Epoch:  121 Loss:  0.227462500333786\n",
      "Epoch:  122 Loss:  0.37888991832733154\n",
      "Epoch:  123 Loss:  0.22996847331523895\n",
      "Epoch:  124 Loss:  0.23055675625801086\n",
      "Epoch:  125 Loss:  0.22596900165081024\n",
      "Epoch:  126 Loss:  0.24230974912643433\n",
      "Epoch:  127 Loss:  0.4975040853023529\n",
      "Epoch:  128 Loss:  0.219416543841362\n",
      "Epoch:  129 Loss:  0.3453775644302368\n",
      "Epoch:  130 Loss:  0.31344714760780334\n",
      "Epoch:  131 Loss:  0.2269628793001175\n",
      "Epoch:  132 Loss:  0.25427424907684326\n",
      "Epoch:  133 Loss:  0.24350622296333313\n",
      "Epoch:  134 Loss:  0.20142020285129547\n",
      "Epoch:  135 Loss:  0.17667007446289062\n",
      "Epoch:  136 Loss:  0.2259397804737091\n",
      "Epoch:  137 Loss:  0.22637152671813965\n",
      "Epoch:  138 Loss:  0.23365484178066254\n",
      "Epoch:  139 Loss:  0.2410248965024948\n",
      "Epoch:  140 Loss:  0.08635871857404709\n",
      "Epoch:  141 Loss:  0.2653621435165405\n",
      "Epoch:  142 Loss:  0.3471423387527466\n",
      "Epoch:  143 Loss:  0.38273710012435913\n",
      "Epoch:  144 Loss:  0.05117644742131233\n",
      "Epoch:  145 Loss:  0.1204209178686142\n",
      "Epoch:  146 Loss:  0.22223132848739624\n",
      "Epoch:  147 Loss:  0.2627263069152832\n",
      "Epoch:  148 Loss:  0.36873310804367065\n",
      "Epoch:  149 Loss:  0.2805003821849823\n",
      "Epoch:  150 Loss:  0.22508195042610168\n",
      "Epoch:  151 Loss:  0.37888413667678833\n",
      "Epoch:  152 Loss:  0.22896888852119446\n",
      "Epoch:  153 Loss:  0.22682616114616394\n",
      "Epoch:  154 Loss:  0.2589767277240753\n",
      "Epoch:  155 Loss:  0.2340158075094223\n",
      "Epoch:  156 Loss:  0.1671622395515442\n",
      "Epoch:  157 Loss:  0.0361952930688858\n",
      "Epoch:  158 Loss:  0.22732265293598175\n",
      "Epoch:  159 Loss:  0.23091323673725128\n",
      "Epoch:  160 Loss:  0.1767420768737793\n",
      "Epoch:  161 Loss:  0.2312765121459961\n",
      "Epoch:  162 Loss:  0.05475187301635742\n",
      "Epoch:  163 Loss:  0.27263662219047546\n",
      "Epoch:  164 Loss:  0.2773783802986145\n",
      "Epoch:  165 Loss:  0.23858965933322906\n",
      "Epoch:  166 Loss:  0.16949962079524994\n",
      "Epoch:  167 Loss:  0.058320801705121994\n",
      "Epoch:  168 Loss:  0.2288656234741211\n",
      "Epoch:  169 Loss:  0.22943490743637085\n",
      "Epoch:  170 Loss:  0.23515552282333374\n",
      "Epoch:  171 Loss:  0.3541320562362671\n",
      "Epoch:  172 Loss:  0.22767332196235657\n",
      "Epoch:  173 Loss:  0.23532775044441223\n",
      "Epoch:  174 Loss:  0.18457108736038208\n",
      "Epoch:  175 Loss:  0.1438266485929489\n",
      "Epoch:  176 Loss:  0.20488396286964417\n",
      "Epoch:  177 Loss:  0.07211174815893173\n",
      "Epoch:  178 Loss:  0.2905236780643463\n",
      "Epoch:  179 Loss:  0.22466343641281128\n",
      "Epoch:  180 Loss:  0.22458308935165405\n",
      "Epoch:  181 Loss:  0.18255726993083954\n",
      "Epoch:  182 Loss:  0.2625407576560974\n",
      "Epoch:  183 Loss:  0.2635338604450226\n",
      "Epoch:  184 Loss:  0.3127376437187195\n",
      "Epoch:  185 Loss:  0.085069440305233\n",
      "Epoch:  186 Loss:  0.2508202791213989\n",
      "Epoch:  187 Loss:  0.5015794038772583\n",
      "Epoch:  188 Loss:  0.23424164950847626\n",
      "Epoch:  189 Loss:  0.23136936128139496\n",
      "Epoch:  190 Loss:  0.2235698103904724\n",
      "Epoch:  191 Loss:  0.27051350474357605\n",
      "Epoch:  192 Loss:  0.24922595918178558\n",
      "Epoch:  193 Loss:  0.24284151196479797\n",
      "Epoch:  194 Loss:  0.23075461387634277\n",
      "Epoch:  195 Loss:  0.2321813851594925\n",
      "Epoch:  196 Loss:  0.23178145289421082\n",
      "Epoch:  197 Loss:  0.30869829654693604\n",
      "Epoch:  198 Loss:  0.16558152437210083\n",
      "Epoch:  199 Loss:  0.23821330070495605\n",
      "Epoch:  200 Loss:  0.22749784588813782\n",
      "Epoch:  201 Loss:  0.3064939081668854\n",
      "Epoch:  202 Loss:  0.22363299131393433\n",
      "Epoch:  203 Loss:  0.222642183303833\n",
      "Epoch:  204 Loss:  0.3712620735168457\n",
      "Epoch:  205 Loss:  0.28814512491226196\n",
      "Epoch:  206 Loss:  0.23511852324008942\n",
      "Epoch:  207 Loss:  0.3851923942565918\n",
      "Epoch:  208 Loss:  0.053684357553720474\n",
      "Epoch:  209 Loss:  0.023756710812449455\n",
      "Epoch:  210 Loss:  0.22297678887844086\n",
      "Epoch:  211 Loss:  0.0366043895483017\n",
      "Epoch:  212 Loss:  0.2480013370513916\n",
      "Epoch:  213 Loss:  0.16615110635757446\n",
      "Epoch:  214 Loss:  0.3257637023925781\n",
      "Epoch:  215 Loss:  0.22729839384555817\n",
      "Epoch:  216 Loss:  0.2307138741016388\n",
      "Epoch:  217 Loss:  0.2262171357870102\n",
      "Epoch:  218 Loss:  0.224928081035614\n",
      "Epoch:  219 Loss:  0.229335755109787\n",
      "Epoch:  220 Loss:  0.24398377537727356\n",
      "Epoch:  221 Loss:  0.09236608445644379\n",
      "Epoch:  222 Loss:  0.24596421420574188\n",
      "Epoch:  223 Loss:  0.30485665798187256\n",
      "Epoch:  224 Loss:  0.21254409849643707\n",
      "Epoch:  225 Loss:  0.24650880694389343\n",
      "Epoch:  226 Loss:  0.23794110119342804\n",
      "Epoch:  227 Loss:  0.2509540021419525\n",
      "Epoch:  228 Loss:  0.2510441243648529\n",
      "Epoch:  229 Loss:  0.2856725752353668\n",
      "Epoch:  230 Loss:  0.39891761541366577\n",
      "Epoch:  231 Loss:  0.25170761346817017\n",
      "Epoch:  232 Loss:  0.07984037697315216\n",
      "Epoch:  233 Loss:  0.22370807826519012\n",
      "Epoch:  234 Loss:  0.42209064960479736\n",
      "Epoch:  235 Loss:  0.2232418805360794\n",
      "Epoch:  236 Loss:  0.2550526261329651\n",
      "Epoch:  237 Loss:  0.289959192276001\n",
      "Epoch:  238 Loss:  0.31569352746009827\n",
      "Epoch:  239 Loss:  0.2873195707798004\n",
      "Epoch:  240 Loss:  0.11611798405647278\n",
      "Epoch:  241 Loss:  0.2997053563594818\n",
      "Epoch:  242 Loss:  0.299737811088562\n",
      "Epoch:  243 Loss:  0.12194749712944031\n",
      "Epoch:  244 Loss:  0.26116129755973816\n",
      "Epoch:  245 Loss:  0.24022772908210754\n",
      "Epoch:  246 Loss:  0.40638211369514465\n",
      "Epoch:  247 Loss:  0.14547352492809296\n",
      "Epoch:  248 Loss:  0.23611068725585938\n",
      "Epoch:  249 Loss:  0.2598019242286682\n",
      "Epoch:  250 Loss:  0.30963513255119324\n",
      "Epoch:  251 Loss:  0.08055763691663742\n",
      "Epoch:  252 Loss:  0.4575413465499878\n",
      "Epoch:  253 Loss:  0.3316994309425354\n",
      "Epoch:  254 Loss:  0.2456403523683548\n",
      "Epoch:  255 Loss:  0.3237036168575287\n",
      "Epoch:  256 Loss:  0.2773040235042572\n",
      "Epoch:  257 Loss:  0.25127851963043213\n",
      "Epoch:  258 Loss:  0.22689573466777802\n",
      "Epoch:  259 Loss:  0.30042776465415955\n",
      "Epoch:  260 Loss:  0.3270726799964905\n",
      "Epoch:  261 Loss:  0.23526865243911743\n",
      "Epoch:  262 Loss:  0.2859097421169281\n",
      "Epoch:  263 Loss:  0.23438940942287445\n",
      "Epoch:  264 Loss:  0.38126495480537415\n",
      "Epoch:  265 Loss:  0.22761593759059906\n",
      "Epoch:  266 Loss:  0.0476812906563282\n",
      "Epoch:  267 Loss:  0.2566922605037689\n",
      "Epoch:  268 Loss:  0.22508388757705688\n",
      "Epoch:  269 Loss:  0.04964785277843475\n",
      "Epoch:  270 Loss:  0.22549939155578613\n",
      "Epoch:  271 Loss:  0.22415119409561157\n",
      "Epoch:  272 Loss:  0.23708279430866241\n",
      "Epoch:  273 Loss:  0.1759539544582367\n",
      "Epoch:  274 Loss:  0.3586946725845337\n",
      "Epoch:  275 Loss:  0.2574637830257416\n",
      "Epoch:  276 Loss:  0.2406778335571289\n",
      "Epoch:  277 Loss:  0.30029064416885376\n",
      "Epoch:  278 Loss:  0.22884713113307953\n",
      "Epoch:  279 Loss:  0.11638527363538742\n",
      "Epoch:  280 Loss:  0.266520619392395\n",
      "Epoch:  281 Loss:  0.3934490978717804\n",
      "Epoch:  282 Loss:  0.3248906433582306\n",
      "Epoch:  283 Loss:  0.09516634792089462\n",
      "Epoch:  284 Loss:  0.2246178835630417\n",
      "Epoch:  285 Loss:  0.4516424536705017\n",
      "Epoch:  286 Loss:  0.23507989943027496\n",
      "Epoch:  287 Loss:  0.3458462059497833\n",
      "Epoch:  288 Loss:  0.22409147024154663\n",
      "Epoch:  289 Loss:  0.06258592009544373\n",
      "Epoch:  290 Loss:  0.23336823284626007\n",
      "Epoch:  291 Loss:  0.2259364128112793\n",
      "Epoch:  292 Loss:  0.07641460746526718\n",
      "Epoch:  293 Loss:  0.2502993643283844\n",
      "Epoch:  294 Loss:  0.2472841441631317\n",
      "Epoch:  295 Loss:  0.22371260821819305\n",
      "Epoch:  296 Loss:  0.240840882062912\n",
      "Epoch:  297 Loss:  0.2848745286464691\n",
      "Epoch:  298 Loss:  0.22485750913619995\n",
      "Epoch:  299 Loss:  0.05256420373916626\n",
      "Epoch:  300 Loss:  0.2520579695701599\n",
      "Epoch:  301 Loss:  0.25933408737182617\n",
      "Epoch:  302 Loss:  0.2413584142923355\n",
      "Epoch:  303 Loss:  0.4117759168148041\n",
      "Epoch:  304 Loss:  0.32321789860725403\n",
      "Epoch:  305 Loss:  0.10526207834482193\n",
      "Epoch:  306 Loss:  0.3137745261192322\n",
      "Epoch:  307 Loss:  0.23136819899082184\n",
      "Epoch:  308 Loss:  0.3789857029914856\n",
      "Epoch:  309 Loss:  0.34110137820243835\n",
      "Epoch:  310 Loss:  0.23301488161087036\n",
      "Epoch:  311 Loss:  0.050767961889505386\n",
      "Epoch:  312 Loss:  0.12266339361667633\n",
      "Epoch:  313 Loss:  0.35244524478912354\n",
      "Epoch:  314 Loss:  0.23279687762260437\n",
      "Epoch:  315 Loss:  0.2332819253206253\n",
      "Epoch:  316 Loss:  0.08172363042831421\n",
      "Epoch:  317 Loss:  0.23953254520893097\n",
      "Epoch:  318 Loss:  0.30801039934158325\n",
      "Epoch:  319 Loss:  0.2547176480293274\n",
      "Epoch:  320 Loss:  0.12058545649051666\n",
      "Epoch:  321 Loss:  0.3339058458805084\n",
      "Epoch:  322 Loss:  0.3683367371559143\n",
      "Epoch:  323 Loss:  0.2377835363149643\n",
      "Epoch:  324 Loss:  0.22985196113586426\n",
      "Epoch:  325 Loss:  0.12250309437513351\n",
      "Epoch:  326 Loss:  0.25539523363113403\n",
      "Epoch:  327 Loss:  0.31420624256134033\n",
      "Epoch:  328 Loss:  0.22712622582912445\n",
      "Epoch:  329 Loss:  0.24132966995239258\n",
      "Epoch:  330 Loss:  0.2758098542690277\n",
      "Epoch:  331 Loss:  0.05659930408000946\n",
      "Epoch:  332 Loss:  0.3326897621154785\n",
      "Epoch:  333 Loss:  0.05293735861778259\n",
      "Epoch:  334 Loss:  0.3189128637313843\n",
      "Epoch:  335 Loss:  0.09419991075992584\n",
      "Epoch:  336 Loss:  0.24741418659687042\n",
      "Epoch:  337 Loss:  0.23080746829509735\n",
      "Epoch:  338 Loss:  0.2864750623703003\n",
      "Epoch:  339 Loss:  0.23104794323444366\n",
      "Epoch:  340 Loss:  0.2773415744304657\n",
      "Epoch:  341 Loss:  0.2651826739311218\n",
      "Epoch:  342 Loss:  0.23006875813007355\n",
      "Epoch:  343 Loss:  0.22852808237075806\n",
      "Epoch:  344 Loss:  0.2289532870054245\n",
      "Epoch:  345 Loss:  0.10633477568626404\n",
      "Epoch:  346 Loss:  0.22899559140205383\n",
      "Epoch:  347 Loss:  0.2431449145078659\n",
      "Epoch:  348 Loss:  0.22734110057353973\n",
      "Epoch:  349 Loss:  0.3493865728378296\n",
      "Epoch:  350 Loss:  0.3494042456150055\n",
      "Epoch:  351 Loss:  0.23085744678974152\n",
      "Epoch:  352 Loss:  0.18004055321216583\n",
      "Epoch:  353 Loss:  0.29616886377334595\n",
      "Epoch:  354 Loss:  0.23105083405971527\n",
      "Epoch:  355 Loss:  0.15427976846694946\n",
      "Epoch:  356 Loss:  0.2459532469511032\n",
      "Epoch:  357 Loss:  0.3034149706363678\n",
      "Epoch:  358 Loss:  0.26846492290496826\n",
      "Epoch:  359 Loss:  0.1730174571275711\n",
      "Epoch:  360 Loss:  0.22779954969882965\n",
      "Epoch:  361 Loss:  0.19551211595535278\n",
      "Epoch:  362 Loss:  0.32146909832954407\n",
      "Epoch:  363 Loss:  0.23715411126613617\n",
      "Epoch:  364 Loss:  0.21354305744171143\n",
      "Epoch:  365 Loss:  0.10318230092525482\n",
      "Epoch:  366 Loss:  0.2297755926847458\n",
      "Epoch:  367 Loss:  0.32333365082740784\n",
      "Epoch:  368 Loss:  0.08362399786710739\n",
      "Epoch:  369 Loss:  0.09748747944831848\n",
      "Epoch:  370 Loss:  0.22286878526210785\n",
      "Epoch:  371 Loss:  0.4183523654937744\n",
      "Epoch:  372 Loss:  0.32431620359420776\n",
      "Epoch:  373 Loss:  0.2950432300567627\n",
      "Epoch:  374 Loss:  0.22786886990070343\n",
      "Epoch:  375 Loss:  0.33057907223701477\n",
      "Epoch:  376 Loss:  0.290142297744751\n",
      "Epoch:  377 Loss:  0.22653494775295258\n",
      "Epoch:  378 Loss:  0.0855502188205719\n",
      "Epoch:  379 Loss:  0.24046246707439423\n",
      "Epoch:  380 Loss:  0.28080058097839355\n",
      "Epoch:  381 Loss:  0.05776917189359665\n",
      "Epoch:  382 Loss:  0.23150184750556946\n",
      "Epoch:  383 Loss:  0.37440723180770874\n",
      "Epoch:  384 Loss:  0.25431492924690247\n",
      "Epoch:  385 Loss:  0.22545528411865234\n",
      "Epoch:  386 Loss:  0.2955152690410614\n",
      "Epoch:  387 Loss:  0.19068150222301483\n",
      "Epoch:  388 Loss:  0.3319551944732666\n",
      "Epoch:  389 Loss:  0.2334943562746048\n",
      "Epoch:  390 Loss:  0.40093111991882324\n",
      "Epoch:  391 Loss:  0.2306634783744812\n",
      "Epoch:  392 Loss:  0.20569314062595367\n",
      "Epoch:  393 Loss:  0.23809576034545898\n",
      "Epoch:  394 Loss:  0.3139665126800537\n",
      "Epoch:  395 Loss:  0.33945509791374207\n",
      "Epoch:  396 Loss:  0.31161320209503174\n",
      "Epoch:  397 Loss:  0.11409613490104675\n",
      "Epoch:  398 Loss:  0.2291904240846634\n",
      "Epoch:  399 Loss:  0.2345956712961197\n",
      "Epoch:  400 Loss:  0.25668835639953613\n",
      "Epoch:  401 Loss:  0.2914780080318451\n",
      "Epoch:  402 Loss:  0.16741180419921875\n",
      "Epoch:  403 Loss:  0.2715551555156708\n",
      "Epoch:  404 Loss:  0.30622559785842896\n",
      "Epoch:  405 Loss:  0.3741920590400696\n",
      "Epoch:  406 Loss:  0.3566589057445526\n",
      "Epoch:  407 Loss:  0.5681317448616028\n",
      "Epoch:  408 Loss:  0.22340035438537598\n",
      "Epoch:  409 Loss:  0.3526932895183563\n",
      "Epoch:  410 Loss:  0.23310153186321259\n",
      "Epoch:  411 Loss:  0.04901403933763504\n",
      "Epoch:  412 Loss:  0.2312258929014206\n",
      "Epoch:  413 Loss:  0.22607506811618805\n",
      "Epoch:  414 Loss:  0.4080744683742523\n",
      "Epoch:  415 Loss:  0.14611095190048218\n",
      "Epoch:  416 Loss:  0.23605741560459137\n",
      "Epoch:  417 Loss:  0.053531255573034286\n",
      "Epoch:  418 Loss:  0.22886759042739868\n",
      "Epoch:  419 Loss:  0.22707965970039368\n",
      "Epoch:  420 Loss:  0.05748862028121948\n",
      "Epoch:  421 Loss:  0.26546189188957214\n",
      "Epoch:  422 Loss:  0.25387731194496155\n",
      "Epoch:  423 Loss:  0.2309166043996811\n",
      "Epoch:  424 Loss:  0.0839223861694336\n",
      "Epoch:  425 Loss:  0.29531311988830566\n",
      "Epoch:  426 Loss:  0.1565103381872177\n",
      "Epoch:  427 Loss:  0.39117684960365295\n",
      "Epoch:  428 Loss:  0.07149052619934082\n",
      "Epoch:  429 Loss:  0.32613295316696167\n",
      "Epoch:  430 Loss:  0.05655692517757416\n",
      "Epoch:  431 Loss:  0.3801287114620209\n",
      "Epoch:  432 Loss:  0.04620910808444023\n",
      "Epoch:  433 Loss:  0.33184343576431274\n",
      "Epoch:  434 Loss:  0.22842741012573242\n",
      "Epoch:  435 Loss:  0.25766611099243164\n",
      "Epoch:  436 Loss:  0.22680619359016418\n",
      "Epoch:  437 Loss:  0.24679654836654663\n",
      "Epoch:  438 Loss:  0.328498512506485\n",
      "Epoch:  439 Loss:  0.23940099775791168\n",
      "Epoch:  440 Loss:  0.22673442959785461\n",
      "Epoch:  441 Loss:  0.228775292634964\n",
      "Epoch:  442 Loss:  0.22998598217964172\n",
      "Epoch:  443 Loss:  0.46209484338760376\n",
      "Epoch:  444 Loss:  0.3609531819820404\n",
      "Epoch:  445 Loss:  0.2316395491361618\n",
      "Epoch:  446 Loss:  0.23046010732650757\n",
      "Epoch:  447 Loss:  0.38348081707954407\n",
      "Epoch:  448 Loss:  0.25857266783714294\n",
      "Epoch:  449 Loss:  0.33919817209243774\n",
      "Epoch:  450 Loss:  0.32515960931777954\n",
      "Epoch:  451 Loss:  0.3744589686393738\n",
      "Epoch:  452 Loss:  0.3824494779109955\n",
      "Epoch:  453 Loss:  0.30744469165802\n",
      "Epoch:  454 Loss:  0.05990276113152504\n",
      "Epoch:  455 Loss:  0.3105722665786743\n",
      "Epoch:  456 Loss:  0.0796680822968483\n",
      "Epoch:  457 Loss:  0.16037458181381226\n",
      "Epoch:  458 Loss:  0.23293621838092804\n",
      "Epoch:  459 Loss:  0.255025714635849\n",
      "Epoch:  460 Loss:  0.23157638311386108\n",
      "Epoch:  461 Loss:  0.2472408562898636\n",
      "Epoch:  462 Loss:  0.2914189100265503\n",
      "Epoch:  463 Loss:  0.29427582025527954\n",
      "Epoch:  464 Loss:  0.2763056755065918\n",
      "Epoch:  465 Loss:  0.24347175657749176\n",
      "Epoch:  466 Loss:  0.2280328869819641\n",
      "Epoch:  467 Loss:  0.2867383062839508\n",
      "Epoch:  468 Loss:  0.23196591436862946\n",
      "Epoch:  469 Loss:  0.2587060034275055\n",
      "Epoch:  470 Loss:  0.0570918507874012\n",
      "Epoch:  471 Loss:  0.30151382088661194\n",
      "Epoch:  472 Loss:  0.21024930477142334\n",
      "Epoch:  473 Loss:  0.10479150712490082\n",
      "Epoch:  474 Loss:  0.4680548906326294\n",
      "Epoch:  475 Loss:  0.25961849093437195\n",
      "Epoch:  476 Loss:  0.28493550419807434\n",
      "Epoch:  477 Loss:  0.06314191222190857\n",
      "Epoch:  478 Loss:  0.2631646394729614\n",
      "Epoch:  479 Loss:  0.3000333607196808\n",
      "Epoch:  480 Loss:  0.2328813076019287\n",
      "Epoch:  481 Loss:  0.24836993217468262\n",
      "Epoch:  482 Loss:  0.08984322100877762\n",
      "Epoch:  483 Loss:  0.3373967707157135\n",
      "Epoch:  484 Loss:  0.23147061467170715\n",
      "Epoch:  485 Loss:  0.24011151492595673\n",
      "Epoch:  486 Loss:  0.4405822157859802\n",
      "Epoch:  487 Loss:  0.0505346842110157\n",
      "Epoch:  488 Loss:  0.33203014731407166\n",
      "Epoch:  489 Loss:  0.23364315927028656\n",
      "Epoch:  490 Loss:  0.22885169088840485\n",
      "Epoch:  491 Loss:  0.30533549189567566\n",
      "Epoch:  492 Loss:  0.3162197768688202\n",
      "Epoch:  493 Loss:  0.4339493215084076\n",
      "Epoch:  494 Loss:  0.2315187156200409\n",
      "Epoch:  495 Loss:  0.2687693238258362\n",
      "Epoch:  496 Loss:  0.3206366002559662\n",
      "Epoch:  497 Loss:  0.22528035938739777\n",
      "Epoch:  498 Loss:  0.2486322969198227\n",
      "Epoch:  499 Loss:  0.13883191347122192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 500\n",
    "X_Train = torch.from_numpy(X_Train.to_numpy()).float()\n",
    "X_Test = torch.from_numpy(X_Test.to_numpy()).float()\n",
    "y = torch.from_numpy(y.to_numpy()).float()\n",
    "\n",
    "train_data_loader = DataLoader(TensorDataset(X_Train, y), batch_size=4, shuffle=True)\n",
    "test_data_loader = DataLoader(TensorDataset(X_Train, y), batch_size=1, shuffle=True)\n",
    "\n",
    "print(X_Train)\n",
    "\n",
    "for n in range(epoch):\n",
    "    for x, y_test in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = nn.functional.mse_loss(y_pred, y_test)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: \", n, \"Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.621773288439955\n"
     ]
    }
   ],
   "source": [
    "num_right = 0\n",
    "for x, y_test in test_data_loader:\n",
    "    y_pred = model(x)\n",
    "    ypred = int(y_pred.round().item())\n",
    "    if ypred == y_test.item():\n",
    "        num_right += 1\n",
    "\n",
    "print(\"Accuracy: \", num_right / len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for n in X_Test:\n",
    "    predictions.append(int(model(n).round().item()))\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
